{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":102085,"databundleVersionId":12294996,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/modu-ds-4-house-prices/train.csv')\ntest = pd.read_csv('/kaggle/input/modu-ds-4-house-prices/test.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"## Basic Structure","metadata":{}},{"cell_type":"code","source":"for column in train.columns:\n    if column not in test.columns:\n        print(column)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Target column: SalePrice","metadata":{}},{"cell_type":"code","source":"print('Train Shape:', train.shape)\nprint('Test  Shape:', test.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"house_df = train.copy()\nhouse_df.head(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('\\nFeatures type\\n',house_df.dtypes.value_counts())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import missingno as msno\nmsno.matrix(df = house_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"isnull_series = house_df.isnull().sum()\nprint('\\nNull Count:\\n',isnull_series[isnull_series > 0].sort_values(ascending=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Null 값이 포함된 열 선택\ntrain_null_columns = house_df.columns[house_df.isnull().any(axis=0)]\nprint(\"Null 값이 포함된 칼럼:\", train_null_columns.size)\nprint(train_null_columns)\n\n# Null 값이 포함되지 않은 열 선택\ntrain_not_null_columns = house_df.columns[house_df.notnull().all(axis=0)]\nprint(\"\\nNull 값이 없는 칼럼:\", train_not_null_columns.size)\nprint(train_not_null_columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Null 값이 포함된 열 선택\ntest_null_columns = test.columns[test.isnull().any(axis=0)]\nprint(\"Null 값이 포함된 칼럼:\", test_null_columns.size)\nprint(test_null_columns)\n\n# Null 값이 포함되지 않은 열 선택\ntest_not_null_columns = test.columns[test.notnull().all(axis=0)]\nprint(\"\\nNull 값이 없는 칼럼:\", test_not_null_columns.size)\nprint(test_not_null_columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for column in train_not_null_columns:\n    if column not in test_not_null_columns:\n        print(column)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for column in train_null_columns:\n    if column not in test_null_columns:\n        print(column)\n        print(\"Is it not Null in test: \",column in test_not_null_columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"house_df[['MasVnrArea','Electrical']].isna().mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Null Anaysis\n\n* Test 데이터 셋과 Train 데이터 셋에서 Not-null 값이 포함된 칼럼은 Target Column을 제외하고 동일하다.\n* Null 값을 포함하고 있는 칼럼은 train set에서 총 19개로, 그 중 두 개의 칼럼은 Test 데이터에서는 Not Null이다.\n* Train Data 기준으로 Not Null 값을 포함하고 있는 칼럼은 총 62개이고 독립변수 61개, 종속변수는 1개이다.\n* Train Data 기준으로 Not Null 값을 포함하고 있는 칼럼은 총 63개이고 독립변수 63개이다.\n* Train 데이터에서 Null이나 Test 데이터에서는 Not Null인 칼럼은 결측치 처리 방안을 고려해 분석에 사용될 예정이다.","metadata":{}},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"markdown","source":"## Target Colum\n\n* Log-scaling process\n* reason: make it follow normal distribution before regression","metadata":{}},{"cell_type":"code","source":"plt.title('Original Sale Price Histogram')\nplt.xticks(rotation=15)\nsns.histplot(house_df['SalePrice'], kde=True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.title('Log Transformed Sale Price Histogram')\nlog_SalePrice = np.log1p(house_df['SalePrice'])\nsns.histplot(log_SalePrice, kde=True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SalePrice 로그 변환\noriginal_SalePrice = house_df['SalePrice'] #기존의 가격정보 저장\nhouse_df['SalePrice'] = np.log1p(house_df['SalePrice']) #스케일링된 값으로 갱신","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## X_features\n\n* Null Value가 많은 칼럼은 삭제(기준: Null Ratio 10% 이상)\n* Null Value가 적은 범주형 칼럼은 Null여부를 새로운 칼럼으로 저장\n* Electrical: Test 데이셋에서는 Not Null 이므로 최빈값으로 대체\n* 숫자형 데이터 왜도가 높은 피쳐는 log 스케일을 적용\n* 숫자형 데이터에 Standard Scaling 적용\n* 원본 데이터를 99% 이상 설명할 수 있는 주성분 사용(Features -2)\n* 범주형 데이터이나 실제로 숫자형을 담고 있는 경우 숫자형으로 변환\n* 범주형 데이터는 One-hot Encoding 적용(Null Value는 별도의 범주로 지정)","metadata":{}},{"cell_type":"markdown","source":"## Null Value 처리","metadata":{}},{"cell_type":"code","source":"top_freqs = []\n\nfor col in house_df.columns:\n    top_freq = house_df[col].value_counts(normalize=True, dropna=False).values[0]\n    top_freqs.append((col, top_freq))\n\n# 정리해서 보기 좋게 출력\ntop_freq_df = pd.DataFrame(top_freqs, columns=['Columxn', 'TopFreqRatio'])\ntop_freq_df = top_freq_df.sort_values(by='TopFreqRatio', ascending=False)\n\n# 90% 이상인 피처만 필터링\n# print(top_freq_df[top_freq_df['TopFreqRatio'] > 0.9])\n\nTopFrepCols = top_freq_df[top_freq_df['TopFreqRatio'] > 0.9]['Columxn'].tolist()\nTopFrepCols","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"null_ratio = house_df[train_null_columns].isna().mean()\ndrop_columns = null_ratio[null_ratio > 0.1].index.tolist() #Null 값의 비율이 10% 초과\ndrop_columns = drop_columns + TopFrepCols\ndrop_columns.append('Id')\n\nprint(drop_columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"house_df.drop(drop_columns, axis = 1 , inplace = True)\nhouse_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(list(set(drop_columns)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"house_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Columns Reduction: 81 - 27 = 54","metadata":{}},{"cell_type":"code","source":"msno.matrix(df = house_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 종속변수 정의\ny_target = house_df['SalePrice']\n\n# Null 값을 가진 컬럼만 추출\nnull_cols = house_df.columns[house_df.isnull().any()]\n\n# 시각화: 3xN 형태로 subplot 그리기\nn_cols = 3\nn_rows = (len(null_cols) + n_cols - 1) // n_cols\n\nplt.figure(figsize=(6 * n_cols, 5 * n_rows))\n\nfor i, col in enumerate(null_cols):\n    # Null 여부를 기준으로 Boxplot 시각화\n    plt.subplot(n_rows, n_cols, i + 1)\n    sns.boxplot(x=house_df[col].isnull(), y=y_target)\n    plt.title(f\"Null 여부에 따른 {col} vs SalePrice\")\n    plt.xlabel(f\"{col} is Null\")\n    plt.ylabel(\"SalePrice\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 나머지 Null Value 처리\n* 숫자형 데이터: 중간값으로 대체\n* 범주형 데이터: 최빈값으로 대체","metadata":{}},{"cell_type":"code","source":"house_df[null_cols].nunique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"house_df[null_cols].dtypes.value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Null 값을 가지는 컬럼에 대해 Null 여부 피처 생성\nfor col in house_df.columns:\n    if house_df[col].isnull().any():\n        house_df[f'{col}_isnull'] = house_df[col].isnull().astype(int).astype('category')\n        if house_df[col].dtype in ['int64', 'float64']: # 숫자형이면 중간값으로 대체\n            house_df[col].fillna(house_df[col].median(), inplace=True)\n        else: # 범주형이면 최빈값으로 대체\n            house_df[col].fillna(house_df[col].mode()[0], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"house_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"msno.matrix(df = house_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* 결측치 전체 처리 완료","metadata":{}},{"cell_type":"markdown","source":"## 숫자형 칼럼 전처리","metadata":{}},{"cell_type":"code","source":"# # 숫자형 컬럼만 추출\n# num_cols = house_df.select_dtypes(include=['number'])\n# df = house_df[num_cols]\n\n# # 서브플롯 설정\n# fig, axes = plt.subplots(nrows = 6, ncols = 6, figsize=(20, 20))\n# axes = axes.flatten()  # 2D 배열을 1D로\n\n# # 컬럼별로 히스토그램 그리기\n# for i, col in enumerate(num_cols):\n#     if i < len(axes):\n#         sns.histplot(data = df, x = col, ax = axes[i], kde = True, bins = 50)\n#         axes[i].set_title(col)\n#     else:\n#         # 만약 36개보다 숫자형 컬럼이 더 많다면, 이후 plot은 무시\n#         break\n\n# # 남은 빈 subplot은 꺼버리기\n# for j in range(len(num_cols), len(axes)):\n#     fig.delaxes(axes[j])\n\n# plt.tight_layout()\n# plt.show()\n# plt.savefig(\"Features_distrubution.png\", dpi=300)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 숫자형 데이터\n* 정말로 연속적인 분포를 갖는 변수도 포함돼 있음.\n* 대부분의 피쳐는 정규분포를 따르지 않는다.\n* 일부 피쳐의 형식은 숫자형이지만 범주형에 대한되는 것들도 보인다.(기준: 데이터 값의 유형 15개 미만)\n* 연속적, 이산적인 값을 가지고 있지만, 데이터가 왼쪽으로 치우진, Right-squid 형태의 피쳐도 존재한다.","metadata":{}},{"cell_type":"code","source":"# # 숫자형 독립변수 선택 \n# num_features = house_df.select_dtypes(include=['number']).columns\n\n# # 종속변수 (y)\n# y = house_df['SalePrice']\n\n# # 서브플롯 설정\n# fig, axes = plt.subplots(nrows=6, ncols=6, figsize=(20, 20))\n# axes = axes.flatten()\n\n# # 산점도 그리기\n# for i, col in enumerate(num_features):\n#     if i < len(axes):\n#         sns.scatterplot(x=house_df[col], y=y, ax=axes[i], alpha=0.5)\n#         axes[i].set_title(f'{col} vs SalePrice', fontsize=10)\n#         axes[i].set_xlabel(col)\n#         axes[i].set_ylabel('SalePrice')\n#     else:\n#         break\n\n# # 남은 subplot 제거\n# for j in range(len(num_features), len(axes)):\n#     fig.delaxes(axes[j])\n\n# plt.tight_layout()\n# plt.show()\n# plt.savefig(\"X,y.png\", dpi=300)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. 숫자형 컬럼 전체 추출\nnum_cols = house_df.select_dtypes(include=['int64', 'float64']).columns\n\n# 2. 이산형 변수 필터링 (int형 + 유일값이 적은 컬럼)\ndiscrete_cols = [col for col in num_cols \n                 if pd.api.types.is_integer_dtype(house_df[col]) and \n                    house_df[col].nunique() <= 15]\n\nprint(\"이산형 변수:\", discrete_cols)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(discrete_cols)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 이산형 변수 전처리\n* 숫자형이나 범주형에 해당되는 칼럼들: 범주형 칼럼으로 변환","metadata":{}},{"cell_type":"markdown","source":"### PoolArea\n* 숫자형 데이터 > 범주형 데이터\n* 처리 원인: Pool이 없는 집이 대부분이지만, 있는 경우 그에 따른 가격변환 관측됨","metadata":{}},{"cell_type":"code","source":"house_df['PoolArea'].unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"house_df['PoolArea'] = (house_df['PoolArea'] > 0).astype(int).astype('category')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"house_df['PoolArea'].unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"discrete_cols.remove(\"PoolArea\")\ndiscrete_cols #숫자형 변수이지만 실질적으로는 범주형 변수","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"house_df[discrete_cols] = house_df[discrete_cols].astype('category') #숫자형 > 범주형 변환","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"house_df.dtypes.apply(lambda x: x.name).value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 연속형 칼럼 전처리\n\n* 0이 대부분인 칼럼: 0은 하나의 그룹, 0이 아닌 값은 연속형 값으로 따로 분류(기준: 0값의 비율 > 5%)\n* 0이 아닌 값만 로그 변환","metadata":{}},{"cell_type":"code","source":"# 연속형 변수 = 숫자형 중 float형\ncontinuous_cols = [col for col in num_cols \n                   if (pd.api.types.is_float_dtype(house_df[col]) or \n                       house_df[col].nunique() > 15)]\n\nprint(\"연속형 변수:\", continuous_cols)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 숫자형 변수 24개 추출 (예: house_df에서)\n# numeric_cols 는 24개의 숫자형 변수 리스트라고 가정\nnumeric_cols = house_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n# 종속변수 (예: y_target) 지정\n# 예시에서는 house_df['SalePrice'] 를 종속변수라고 가정\ny = house_df['SalePrice']\n\n# 서브플롯 설정\nfig, axes = plt.subplots(nrows=6, ncols=4, figsize=(16, 24))\naxes = axes.flatten()\n\n# 각 숫자형 변수에 대해 Scatter Plot 그리기\nfor i, col in enumerate(numeric_cols[:24]):\n    sns.scatterplot(x=house_df[col], y=y, ax=axes[i])\n    axes[i].set_title(f'{col} vs SalePrice', fontsize=10)\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('SalePrice')\n\n# 빈 subplot 제거 (혹시 숫자형 변수가 24개보다 적을 경우 대비)\nfor j in range(len(numeric_cols), len(axes)):\n    fig.delaxes(axes[j])\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zero_ratio_cols = []\n\nfor col in house_df.columns:\n    if pd.api.types.is_numeric_dtype(house_df[col]):  # 숫자형만\n        zero_ratio = (house_df[col] == 0).mean()\n        if zero_ratio >= 0.05: #기준 비율: 5%\n            zero_ratio_cols.append((col, zero_ratio))\n\n# 결과 보기\nfor col, ratio in zero_ratio_cols:\n    print(f\"{col}: {ratio:.2%}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zero_ratio_cols = [col for col, _ in zero_ratio_cols]\nhouse_df[zero_ratio_cols].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in zero_ratio_cols: # 0의 비율이 5%이상 칼럼에 로그 스케일 적용\n    house_df[col] = house_df[col].apply(lambda x: np.log1p(x) if x > 0 else 0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 숫자형 변수 24개 추출 (예: house_df에서)\n# numeric_cols 는 24개의 숫자형 변수 리스트라고 가정\nnumeric_cols = house_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n# 종속변수 (예: y_target) 지정\n# 예시에서는 house_df['SalePrice'] 를 종속변수라고 가정\ny = house_df['SalePrice']\n\n# 서브플롯 설정\nfig, axes = plt.subplots(nrows=6, ncols=4, figsize=(16, 24))\naxes = axes.flatten()\n\n# 각 숫자형 변수에 대해 Scatter Plot 그리기\nfor i, col in enumerate(numeric_cols[:24]):\n    sns.scatterplot(x=house_df[col], y=y, ax=axes[i])\n    axes[i].set_title(f'{col} vs SalePrice', fontsize=10)\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('SalePrice')\n\n# 빈 subplot 제거 (혹시 숫자형 변수가 24개보다 적을 경우 대비)\nfor j in range(len(numeric_cols), len(axes)):\n    fig.delaxes(axes[j])\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* 로그 스케일링 적용 후 분포 자체의 변화는 없지만, x의 값의 범위는 축소된 점이 관측됨.","metadata":{}},{"cell_type":"markdown","source":"## 왜도 보정\n\n* 정규분포를 따르지 않는 변수들의 왜도를 최대한 따를 수 있도록 보정\n* 왜도가 높은 피쳐에 로그 스케일을 적용할 경우 약 50%의 칼럼의 왜도를 보정.\n* 나머지 50%의 피쳐는 로그 스케일 후에 여전히 왜도가 높은데, 모델에 따라 별로의 처리가 요구된다.\n* Rilear Regression Model에는 다소 영향을 끼치나, 회귀 트리 기반 모델에 끼치는 영향은 적다.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import skew\n\n# 숫자형 데이터 중 실제로 모든 값이 숫자인 컬럼만 필터링\npure_numeric_cols = house_df[numeric_cols].select_dtypes(include=[np.number]).columns\n# house_df에 칼럼 index를 [ ]로 입력하면 해당하는 칼럼 데이터 세트 반환. apply lambda로 skew( ) 호출\nskew_features = house_df[pure_numeric_cols].apply(lambda x : skew(x))\n# skew(왜곡) 정도가 1 이상인 칼럼만 추출.\nskew_features_top = skew_features[abs(skew_features) > 1] # 왜도가 1보다 큰 경우 정규분포를 따르지 않음\nprint(skew_features_top.sort_values(ascending=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.log1p(house_df[skew_features_top.index]).apply(lambda x : skew(x)) <= 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"house_df[skew_features_top.index] = np.log1p(house_df[skew_features_top.index])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 범주형 칼럼 인코딩","metadata":{}},{"cell_type":"code","source":"print('get_dummies() 수행 전 데이터 Shape:', house_df.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"house_df.dtypes.apply(lambda x: x.name).value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 카테고리형, object형 컬럼만 따로 추출\ncat_cols = house_df.select_dtypes(include=['category','object']).columns\nprint(\"Category형 변수 개수: \",len(cat_cols))\nprint(\"Category형 변수 고유값 개수\")\nfor col in cat_cols:\n    print(f\"\\t{col}: {house_df[col].nunique()}개\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"house_df_ohe = pd.get_dummies(house_df, drop_first = True)\nprint('get_dummies() 수행 후 데이터 Shape:', house_df_ohe.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 상관관계 행렬 계산\ncorr_matrix = house_df_ohe.corr()\n\n# 히트맵 시각화\nplt.figure(figsize=(16, 12))  # 크기 조절\nsns.heatmap(corr_matrix, cmap='coolwarm', annot=False, fmt='.2f', linewidths=0.5)\nplt.title('Correlation Heatmap of Features')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr_matrix","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 상관계수 행렬 생성\ncorr_matrix = house_df_ohe.corr().abs()\n\n# 상삼각 행렬만 추출 (자기 자신과의 상관 제외 및 중복 제거)\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n\n# 상관계수 기준 필터링 (예: 0.8 이상인 변수쌍)\nhigh_corr_var = [column for column in upper.columns if any(upper[column] > 0.8)]\n\n# 필터링된 변수로만 다시 상관계수 행렬 생성\nfiltered_corr = house_df_ohe[high_corr_var].corr()\n\n# 히트맵 시각화\nplt.figure(figsize=(12, 10))\nsns.heatmap(filtered_corr, cmap='coolwarm')\nplt.title(\"Highly Correlated Features (|corr| > 0.8)\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 차원 축소여부 검토\n\n* 수행은 해봤지만, 칼럼을 2개 축소하는 효과밖에 없어서 적용하지 않음.","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n\n# 정규화\nscaler = StandardScaler()\ndf = house_df_ohe.select_dtypes(include=['int64', 'float64']) # 숫자형 데이터에만 정규화 적용\nX_scaled = scaler.fit_transform(df)\n\n# PCA\npca = PCA(n_components = 0.99)  # 99% 이상 설명하는 주성분만 사용\nX_pca = pca.fit_transform(X_scaled)\n\nprint(f\"선택된 주성분 개수: {X_pca.shape[1]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* 전체 숫자형 변수 = 22\n* 99% 이상 설명력 있는 숫자형 변수 = 20\n* 차원축소: -2","metadata":{}},{"cell_type":"code","source":"# 설명 분산 비율 (variance ratio)\nexplained_variance_ratio = pca.explained_variance_ratio_\ncumulative_variance = np.cumsum(explained_variance_ratio)\n\n# 시각화\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_variance, marker='o', linestyle='--', color='b')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Explained Variance Ratio')\nplt.title('Explained Variance by Number of Principal Components')\nplt.grid(True)\n\n# 특정 분기점(예: 99%) 표시\nplt.axhline(y=0.99, color='r', linestyle='-')\nplt.axvline(x=np.argmax(cumulative_variance >= 0.99), color='r', linestyle='--')\nplt.text(np.argmax(cumulative_variance >= 0.99)+1, 0.9, '99% cut-off', color='red')\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* 물론 PCA로 연속형 변수 2개를 제거할 수 있지만, 대부분은 범주형 데이터라서 따로 진행하지 않음","metadata":{}},{"cell_type":"markdown","source":"## Outliar 제거","metadata":{}},{"cell_type":"code","source":"plt.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('GrLivArea', fontsize=15)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# GrLivArea와 SalePrice 모두 로그 변환되었으므로 이를 반영한 조건 생성.\ncond1 = house_df_ohe['GrLivArea'] > np.log1p(4000)\ncond2 = house_df_ohe['SalePrice'] < np.log1p(500000)\noutlier_index = house_df_ohe[cond1 & cond2].index\n\nprint('아웃라이어 레코드 index :', outlier_index.values)\nprint('아웃라이어 삭제 전 house_df_ohe shape:', house_df_ohe.shape)\n# DataFrame의 index를 이용하여 아웃라이어 레코드 삭제.\nhouse_df_ohe.drop(outlier_index , axis=0, inplace=True)\nprint('아웃라이어 삭제 후 house_df_ohe shape:', house_df_ohe.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Function Definition","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ndef get_rmse(model):\n    pred = model.predict(X_test)\n    pred_exp = np.expm1(pred)\n    mse = mean_squared_error(np.expm1(y_test) , pred_exp)\n    rmse = np.sqrt(mse)\n    print('{0} 로그 변환된 RMSE: {1}'.format(model.__class__.__name__,np.round(rmse, 3)))\n    print('{0} 로그 변환된 MSE: {1}'.format(model.__class__.__name__,np.round(mse, 3)))\n    return rmse\n\ndef get_rmses(models):\n    rmses = [ ]\n    for model in models:\n        rmse = get_rmse(model)\n        rmses.append(rmse)\n    return rmses","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_top_bottom_coef(model):\n    # coef_ 속성을 기반으로 Series 객체를 생성. index는 컬럼명.\n    coef = pd.Series(model.coef_, index=X_features.columns)\n\n    # + 상위 10개 , - 하위 10개 coefficient 추출하여 반환.\n    coef_high = coef.sort_values(ascending=False).head(10)\n    coef_low = coef.sort_values(ascending=False).tail(10)\n    return coef_high, coef_low","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_coefficient(models):\n    # 3개 회귀 모델의 시각화를 위해 3개의 컬럼을 가지는 subplot 생성\n    fig, axs = plt.subplots(figsize=(24,10),nrows=1, ncols=3)\n    fig.tight_layout()\n    # 입력인자로 받은 list객체인 models에서 차례로 model을 추출하여 회귀 계수 시각화.\n    for i_num, model in enumerate(models):\n        # 상위 10개, 하위 10개 회귀 계수를 구하고, 이를 판다스 concat으로 결합.\n        coef_high, coef_low = get_top_bottom_coef(model)\n        coef_concat = pd.concat( [coef_high , coef_low] )\n        # 순차적으로 ax subplot에 barchar로 표현. 한 화면에 표현하기 위해 tick label 위치와 font 크기 조정.\n        axs[i_num].set_title(model.__class__.__name__+' Coeffiecents', size=25)\n        axs[i_num].tick_params(axis=\"y\",direction=\"in\", pad=-120)\n        for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()):\n            label.set_fontsize(22)\n        sns.barplot(x=coef_concat.values, y=coef_concat.index , ax=axs[i_num])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\ndef get_avg_rmse_cv(models):\n    for model in models:\n        # 분할하지 않고 전체 데이터로 cross_val_score( ) 수행. 모델별 CV RMSE값과 평균 RMSE 출력\n        rmse_list = np.sqrt(-cross_val_score(model, X_features, y_target,\n                                             scoring=\"neg_mean_squared_error\", cv = 5))\n        rmse_avg = np.mean(rmse_list)\n        print('\\n{0} CV RMSE 값 리스트: {1}'.format( model.__class__.__name__, np.round(rmse_list, 3)))\n        print('{0} CV 평균 RMSE 값: {1}'.format( model.__class__.__name__, np.round(rmse_avg, 3)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ndef print_best_params(model, params):\n    grid_model = GridSearchCV(model, param_grid=params,\n                              scoring='neg_mean_squared_error', cv=5)\n    grid_model.fit(X_features, y_target)\n    rmse = np.sqrt(-1* grid_model.best_score_)\n    print('{0} 5 CV 시 최적 평균 RMSE 값: {1}, 최적 alpha:{2}'.format(model.__class__.__name__,\n                                        np.round(rmse, 4), grid_model.best_params_))\n    return grid_model.best_estimator_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 모델의 중요도 상위 20개의 피처명과 그때의 중요도값을 Series로 반환.\ndef get_top_features(model):\n    ftr_importances_values = model.feature_importances_\n    ftr_importances = pd.Series(ftr_importances_values, index=X_features.columns  )\n    ftr_top50 = ftr_importances.sort_values(ascending=False)[:50]\n    return ftr_top50\n\ndef visualize_ftr_importances(models):\n    # 2개 회귀 모델의 시각화를 위해 2개의 컬럼을 가지는 subplot 생성\n    fig, axs = plt.subplots(figsize=(24,10),nrows=1, ncols=2)\n    fig.tight_layout()\n    # 입력인자로 받은 list객체인 models에서 차례로 model을 추출하여 피처 중요도 시각화.\n    for i_num, model in enumerate(models):\n        # 중요도 상위 20개의 피처명과 그때의 중요도값 추출\n        ftr_top20 = get_top_features(model)\n        axs[i_num].set_title(model.__class__.__name__+' Feature Importances', size=25)\n        #font 크기 조정.\n        for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()):\n            label.set_fontsize(22)\n        sns.barplot(x=ftr_top20.values, y=ftr_top20.index , ax=axs[i_num])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_rmse_pred(preds):\n    for key in preds.keys():\n        pred_value = preds[key]\n        mse = mean_squared_error(y_test , pred_value)\n        rmse = np.sqrt(mse)\n        print('{0} 모델의 RMSE: {1}'.format(key, rmse))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\n\n# 개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터를 생성하기 위한 함수.\ndef get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds ):\n    # 지정된 n_folds값으로 KFold 생성.\n    kf = KFold(n_splits=n_folds, shuffle=False)\n    #추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화\n    train_fold_pred = np.zeros((X_train_n.shape[0] ,1 ))\n    test_pred = np.zeros((X_test_n.shape[0],n_folds))\n    print(model.__class__.__name__ , ' model 시작 ')\n\n    for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train_n)):\n        #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 셋 추출\n        print('\\t 폴드 세트: ',folder_counter,' 시작 ')\n        X_tr = X_train_n[train_index]\n        y_tr = y_train_n[train_index]\n        X_te = X_train_n[valid_index]\n\n        #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행.\n        model.fit(X_tr , y_tr)\n        #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장.\n        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1)\n        #입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장.\n        test_pred[:, folder_counter] = model.predict(X_test_n)\n\n    # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성\n    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1)\n\n    #train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터\n    return train_fold_pred , test_pred_mean","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"y_target = house_df_ohe['SalePrice']\nX_features = house_df_ohe.drop('SalePrice',axis=1, inplace=False)\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Linear Regression","metadata":{}},{"cell_type":"markdown","source":"### Hyperparameter Optimization","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\n\n# 앞의 최적화 alpha값으로 학습데이터로 학습, 테스트 데이터로 예측 및 평가 수행.\nlr_reg = LinearRegression()\nlr_reg.fit(X_train, y_train)\nridge_reg = Ridge()\nridge_reg.fit(X_train, y_train)\nlasso_reg = Lasso()\nlasso_reg.fit(X_train, y_train)\n\n# Skew가 높은 피처들을 로그 변환 했으므로 다시 원-핫 인코딩 적용 및 피처/타겟 데이터 셋 생성,\nhouse_df_ohe = pd.get_dummies(house_df)\ny_target = house_df_ohe['SalePrice']\nX_features = house_df_ohe.drop('SalePrice',axis = 1, inplace=False)\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156)\n\n# 피처들을 로그 변환 후 다시 최적 하이퍼 파라미터와 RMSE 출력\nridge_params = { 'alpha':[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] }\nlasso_params = { 'alpha':[0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1,5, 10] }\nbest_ridge = print_best_params(ridge_reg, ridge_params)\nbest_lasso = print_best_params(lasso_reg, lasso_params)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train, predict and evaluate model","metadata":{}},{"cell_type":"code","source":"# 앞의 최적화 alpha값으로 학습데이터로 학습, 테스트 데이터로 예측 및 평가 수행.\nlr_reg = LinearRegression()\nlr_reg.fit(X_train, y_train)\nridge_reg = best_ridge\nridge_reg.fit(X_train, y_train)\nlasso_reg = best_lasso\nlasso_reg.fit(X_train, y_train)\n\n# 모든 모델의 RMSE 출력\nmodels = [lr_reg, ridge_reg, lasso_reg]\nget_rmses(models)\n\n# 모든 모델의 회귀 계수 시각화\nmodels = [lr_reg, ridge_reg, lasso_reg]\nvisualize_coefficient(models)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 앞 예제에서 학습한 lr_reg, ridge_reg, lasso_reg 모델의 CV RMSE값 출력\nmodels = [lr_reg, ridge_reg, lasso_reg]\nget_avg_rmse_cv(models)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tree Regressiors","metadata":{}},{"cell_type":"markdown","source":"###  XGBoost Regressor","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\nxgb_params = {'n_estimators':[1000]}\nxgb_reg = XGBRegressor(n_estimators=1000, learning_rate=0.05,\n                       colsample_bytree=0.5, subsample=0.8)\nbest_xgb = print_best_params(xgb_reg, xgb_params)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### LGBMRegressor","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\n\nlgbm_params = {'n_estimators':[1000]}\nlgbm_reg = LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=4, verbose = -1,\n                         subsample=0.6, colsample_bytree=0.4, reg_lambda=10, n_jobs=-1)\nbest_lgbm = print_best_params(lgbm_reg, lgbm_params)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 앞 예제에서 print_best_params( )가 반환한 GridSearchCV로 최적화된 모델의 피처 중요도 시각화\nmodels = [best_xgb, best_lgbm]\nvisualize_ftr_importances(models)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Stacking Model","metadata":{}},{"cell_type":"code","source":"# 개별 모델의 학습\nridge_reg = Ridge(alpha=10)\nridge_reg.fit(X_train, y_train)\nlasso_reg = Lasso(alpha=0.001)\nlasso_reg.fit(X_train, y_train)\n\n# 개별 모델 예측\nridge_pred = ridge_reg.predict(X_test)\nlasso_pred = lasso_reg.predict(X_test)\n\n# 개별 모델 예측값 혼합으로 최종 예측값 도출\npred = 0.4 * ridge_pred + 0.6 * lasso_pred\npreds = {'최종 혼합': pred,\n         'Ridge': ridge_pred,\n         'Lasso': lasso_pred}\n#최종 혼합 모델, 개별모델의 RMSE 값 출력\nget_rmse_pred(preds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_reg = XGBRegressor(n_estimators=1000, learning_rate=0.05,\n                       colsample_bytree=0.5, subsample=0.8)\nlgbm_reg = LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=4,verbose = -1,\n                         subsample=0.6, colsample_bytree=0.4, reg_lambda=10, n_jobs=-1)\n\nxgb_reg.fit(X_train, y_train)\nlgbm_reg.fit(X_train, y_train)\nxgb_pred = xgb_reg.predict(X_test)\nlgbm_pred = lgbm_reg.predict(X_test)\n\npred = 0.6 * xgb_pred + 0.4 * lgbm_pred\npreds = {'최종 혼합': pred,\n         'XGBM': xgb_pred,\n         'LGBM': lgbm_pred}\n\nget_rmse_pred(preds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Stacking Model","metadata":{}},{"cell_type":"code","source":"# get_stacking_base_datasets( )은 넘파이 ndarray를 인자로 사용하므로 DataFrame을 넘파이로 변환.\nX_train_n = X_train.values\nX_test_n = X_test.values\ny_train_n = y_train.values\n\n# 각 개별 기반(Base)모델이 생성한 학습용/테스트용 데이터 반환.\nridge_train, ridge_test = get_stacking_base_datasets(ridge_reg, X_train_n, y_train_n, X_test_n, 5)\nlasso_train, lasso_test = get_stacking_base_datasets(lasso_reg, X_train_n, y_train_n, X_test_n, 5)\nxgb_train, xgb_test = get_stacking_base_datasets(xgb_reg, X_train_n, y_train_n, X_test_n, 5)\nlgbm_train, lgbm_test = get_stacking_base_datasets(lgbm_reg, X_train_n, y_train_n, X_test_n, 5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 개별 모델이 반환한 학습 및 테스트용 데이터 세트를 Stacking 형태로 결합.\nStack_final_X_train = np.concatenate((ridge_train, lasso_train,\n                                      xgb_train, lgbm_train), axis=1)\nStack_final_X_test = np.concatenate((ridge_test, lasso_test,\n                                     xgb_test, lgbm_test), axis=1)\n\n# 최종 메타 모델은 라쏘 모델을 적용.\nmeta_model_lasso = Lasso(alpha=0.0005)\n\n#기반 모델의 예측값을 기반으로 새롭게 만들어진 학습 및 테스트용 데이터로 예측하고 RMSE 측정.\nmeta_model_lasso.fit(Stack_final_X_train, y_train)\nfinal = meta_model_lasso.predict(Stack_final_X_test)\nmse = mean_squared_error(y_test , final)\nrmse = np.sqrt(mse)\nprint('스태킹 회귀 모델의 최종 RMSE 값은:', rmse)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"test_ohe2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## sample_data를 받아오기\nsample = pd.read_csv('/kaggle/input/modu-ds-4-house-prices/sample_submission.csv')\n\n#최종 모델 예측값\nridge_pred = ridge_reg.predict(test_ohe2)\nlasso_pred = lasso_reg.predict(test_ohe2)\n\n# 개별 모델 예측값 혼합으로 최종 예측값 도출\npred = 0.4 * ridge_pred + 0.6 * lasso_pred\n\n# 학습시 로그변환한 target을 썻으니 결과값을 다시 복원해야합니다\npred_exp = np.expm1(pred)\n\n# sample 파일에 예측 값을 넣어주고\nsample[\"SalePrice\"] = pred_exp\n\n# csv로 저장하기\nsample.to_csv(\"/kaggle/working/submission.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}